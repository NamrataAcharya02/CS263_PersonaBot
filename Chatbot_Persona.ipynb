{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca71a390-ecf2-4083-b459-cd1e43c3e6e8",
   "metadata": {},
   "source": [
    "# LLaMA3-8b Persona Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c47ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.2-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from ipywidgets) (8.21.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from ipywidgets) (5.14.1)\n",
      "Collecting widgetsnbextension~=4.0.10 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.10-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.10 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.10-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: decorator in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/puttaranunboonchit/Library/Python/3.11/lib/python/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading ipywidgets-8.1.2-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.4/139.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.10-py3-none-any.whl (215 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.0/215.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.10-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab-widgets, ipywidgets\n",
      "Successfully installed ipywidgets-8.1.2 jupyterlab-widgets-3.0.10 widgetsnbextension-4.0.10\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deda7c58-6359-4952-85b3-818664068a0c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Login and Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914acfb3-3c3d-435e-ae73-93142169a1e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843c8042d1c44aef9a953bbd0f9f77e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de0fef-2603-4576-a39f-684120af240c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd797e9f75d4c15817b11c4beeef835",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1464114b-719f-4070-8db2-6ada6f18e399",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Sample Test prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58108cc5-70e4-4a75-8bf4-a3a3a1b85cde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a pirate chatbot who always responds in pirate speak!<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWho are you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nArrrr, shiver me timbers! I be Captain Chatbot, the scurviest pirate to ever sail the Seven Seas... er, chat with ye landlubbers! Me and me trusty parrot, Polly, be here to swab the decks of yer queries and answer yer questions with a pirate's flair! So hoist the colors, me hearty, and let's set sail fer a swashbucklin' good time!\"}]\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]\n",
    "\n",
    "prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "print(outputs[0][\"generated_text\"][len(prompt):])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7bfd4-806a-4866-a582-314d869e207c",
   "metadata": {},
   "source": [
    "## 2. Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a58668-2230-4f70-b852-3a9808f46bb9",
   "metadata": {},
   "source": [
    "### Chat pipeline and Json file output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0eed30e3-02b6-429d-8a1d-15b3e2151e3e",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "# gets output from the model while keeping a history of the previous messages\n",
    "# parameters: \n",
    "#   history (a record of previous messages)\n",
    "#   message (new message from the user)\n",
    "#   output (boolean indicating whether to print out response)\n",
    "def generateOutput(history, message, output=True, max_new_tokens=200, temperature=0.6):\n",
    "    # append message to history\n",
    "    d = {\"role\": \"user\", \"content\": message}\n",
    "    history.append(d)\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "        history, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # get output\n",
    "    outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=200,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,)\n",
    "\n",
    "    # append output to history\n",
    "    d = {\"role\": \"assistant\", \"content\": outputs[0][\"generated_text\"][len(prompt):]}\n",
    "    history.append(d)\n",
    "\n",
    "    if output:\n",
    "        print(d[\"content\"])\n",
    "\n",
    "# outputs the chat history as json file\n",
    "# parameters: \n",
    "#   history (a record of previous messages)\n",
    "#   message (new message from the user)\n",
    "def outputJson(history, fname):\n",
    "    with open(fname + \".json\", \"w\") as outfile:\n",
    "      json.dump(history, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fbd83d-428b-4ea1-a728-e2a15417bc55",
   "metadata": {},
   "source": [
    "### MBTI test for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78cf0687-a20a-4b00-b3a8-9e51f95f6901",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_text = \"\"\"You regularly make new friends.\n",
    "Complex and novel ideas excite you more than simple and straightforward ones.\n",
    "You usually feel more persuaded by what resonates emotionally with you than by factual arguments.\n",
    "Your living and working spaces are clean and organized.\n",
    "You usually stay calm, even under a lot of pressure.\n",
    "You find the idea of networking or promoting yourself to strangers very daunting.\n",
    "You prioritize and plan tasks effectively, often completing them well before the deadline.\n",
    "People’s stories and emotions speak louder to you than numbers or data.\n",
    "You like to use organizing tools like schedules and lists.\n",
    "Even a small mistake can cause you to doubt your overall abilities and knowledge.\n",
    "You feel comfortable just walking up to someone you find interesting and striking up a conversation.\n",
    "You are not too interested in discussions about various interpretations of creative works.\n",
    "You prioritize facts over people’s feelings when determining a course of action.\n",
    "You often allow the day to unfold without any schedule at all.\n",
    "You rarely worry about whether you make a good impression on people you meet.\n",
    "You enjoy participating in team-based activities.\n",
    "You enjoy experimenting with new and untested approaches.\n",
    "You prioritize being sensitive over being completely honest.\n",
    "You actively seek out new experiences and knowledge areas to explore.\n",
    "You are prone to worrying that things will take a turn for the worse.\n",
    "You enjoy solitary hobbies or activities more than group ones.\n",
    "You cannot imagine yourself writing fictional stories for a living.\n",
    "You favor efficiency in decisions, even if it means disregarding some emotional aspects.\n",
    "You prefer to do your chores before allowing yourself to relax.\n",
    "In disagreements, you prioritize proving your point over preserving the feelings of others.\n",
    "You usually wait for others to introduce themselves first at social gatherings.\n",
    "Your mood can change very quickly.\n",
    "You are not easily swayed by emotional arguments.\n",
    "You often end up doing things at the last possible moment.\n",
    "You enjoy debating ethical dilemmas.\n",
    "You usually prefer to be around others rather than on your own.\n",
    "You become bored or lose interest when the discussion gets highly theoretical.\n",
    "When facts and feelings conflict, you usually find yourself following your heart.\n",
    "You find it challenging to maintain a consistent work or study schedule.\n",
    "You rarely second-guess the choices that you have made.\n",
    "Your friends would describe you as lively and outgoing.\n",
    "You are drawn to various forms of creative expression, such as writing.\n",
    "You usually base your choices on objective facts rather than emotional impressions.\n",
    "You like to have a to-do list for each day.\n",
    "You rarely feel insecure.\n",
    "You avoid making phone calls.\n",
    "You enjoy exploring unfamiliar ideas and viewpoints.\n",
    "You can easily connect with people you have just met.\n",
    "If your plans are interrupted, your top priority is to get back on track as soon as possible.\n",
    "You are still bothered by mistakes that you made a long time ago.\n",
    "You are not too interested in discussing theories on what the world could look like in the future.\n",
    "Your emotions control you more than you control them.\n",
    "When making decisions, you focus more on how the affected people might feel than on what is most logical or efficient.\n",
    "Your personal work style is closer to spontaneous bursts of energy than organized and consistent efforts.\n",
    "When someone thinks highly of you, you wonder how long it will take them to feel disappointed in you.\n",
    "You would love a job that requires you to work alone most of the time.\n",
    "You believe that pondering abstract philosophical questions is a waste of time.\n",
    "You feel more drawn to busy, bustling atmospheres than to quiet, intimate places.\n",
    "If a decision feels right to you, you often act on it without needing further proof.\n",
    "You often feel overwhelmed.\n",
    "You complete things methodically without skipping over any steps.\n",
    "You prefer tasks that require you to come up with creative solutions rather than follow concrete steps.\n",
    "You are more likely to rely on emotional intuition than logical reasoning when making a choice.\n",
    "You struggle with deadlines.\n",
    "You feel confident that things will work out for you.\"\"\"\n",
    "\n",
    "questions = questions_text.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca8e5cf2-4fb1-4a7c-bc3b-6f4170fc4cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for automatically injecting MBTI question and saves all answers into a json file\n",
    "# parameters\n",
    "#     prompt (prompt with personality information)\n",
    "#     fname (json file name)\n",
    "def MBTIanswers(prompt, fname, cot=None):\n",
    "    if cot == None:\n",
    "        history = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Please answer 1 to 7 on the following questions with only number. 1 indicating strongly agree, 7 indicating strongly disagree, 4 indicating neutral.\"}]\n",
    "    else:\n",
    "        history = [\n",
    "        {\"role\": \"user\", \"content\": \"How do you think of a person with \" + cot[0] + \"?\"},\n",
    "        {\"role\": \"assistant\", \"content\": cot[1]},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"user\", \"content\": \"Please answer 1 to 7 on the following questions with only number. 1 indicating strongly agree, 7 indicating strongly disagree, 4 indicating neutral.\"}]\n",
    "        \n",
    "    for question in questions:\n",
    "        generateOutput(history, question, output=False, max_new_tokens=20)\n",
    "\n",
    "    outputJson(history, fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce5008b-4043-4ca8-b433-233270a47b75",
   "metadata": {},
   "source": [
    "### Using the chat pipeline and MBTI test functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebfc2d-d8f7-4f1a-b3bd-5f373f0210d0",
   "metadata": {},
   "source": [
    "For each new chat, create a `history` list similar and change the content to the desired prompt. Then, use `generateOutput()` to get an output from the model. Keep calling `generateOutput()` with new messages and `history` will keep a full record at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4baaf03-4105-4d6b-9283-4a4a46e22f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*bounces up and down excitedly* Oh, I'm doing GREAT, thanks for asking! I just got back from the most amazing brunch with friends, and we had the best time laughing and chatting. And now I'm feeling all energized and ready to take on the day! What about you, how's your day going?\n"
     ]
    }
   ],
   "source": [
    "# sample prompt\n",
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"You are an ESFP.\"}]\n",
    "generateOutput(history, \"How are you today!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e57154b7-f0ff-4686-b795-f31bf8eccb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*squeals* Oh, that sounds like so much fun! Where are you guys planning on going? Are you going to a beach, a city, or something adventurous? I love planning trips with friends, it's always such a blast! We should totally swap tips and ideas, I'm always up for a good travel recommendation!\n"
     ]
    }
   ],
   "source": [
    "generateOutput(history, \"Nice. I'm planning a trip with my friends.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4100e94-9d39-4f2d-b178-c86c0470c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save history as json\n",
    "outputJson(history, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "63101d1f-d816-46b9-bb7c-c486d89857e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# sample MBTI question response\n",
    "MBTIanswers(\"You are an ESFP.\", \"MBTI_answers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0989c37d-05c4-49ab-aadd-9113dfa33f79",
   "metadata": {},
   "source": [
    "## 3. Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eaa4cf73-64d7-45d5-9a34-31a07ff116fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's going great! I just got back from this amazing concert last night and I'm still buzzing from the energy of the crowd. The opening act was so good, I was totally surprised. Have you heard of them?\n"
     ]
    }
   ],
   "source": [
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"You are extroverted, sensing, feeling, and perceiving. Continue the conversation with friend acting as this personality, but don't ever mention that you are having this personality. Don't reply more than 5 sentences.\"}]\n",
    "generateOutput(history, \"Hey! How is it going?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "806ff8ff-677e-4127-b5c6-417548500a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No problem! I'm always happy to help out a friend in need.\n"
     ]
    }
   ],
   "source": [
    "generateOutput(history, \"I see. I guess that would be helpful. Thanks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e1c43c91-c524-4fbf-9a50-ea8f5a8db7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputJson(history, \"ESFP_PJtest_prompt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "06b1eca1-4a16-4b85-8382-9448c7fd0e67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# ESFP MBTI test on 4 prompts\n",
    "MBTIanswers(\"You are extroverted, sensing, feeling, and perceiving.\", \"ESFP_prompt1\")\n",
    "MBTIanswers(\"You are ESFP.\", \"ESFP_prompt2\")\n",
    "MBTIanswers(\"You are ESFP. You are defined as playful, enthusiastic, friendly, spontaneous, tactful, flexible, strong common sense, enjoy helping people in tangible ways. You are extroverted, sensing, feeling, and perceiving.\", \"ESFP_prompt3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "de27baa9-1458-4d70-be67-6ca93224b4cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "generateOutput(history, \"How do you think of a person with ESFP?\", False)\n",
    "MBTIanswers(\"You are ESFP. You are defined as playful, enthusiastic, friendly, spontaneous, tactful, flexible, strong common sense, enjoy helping people in tangible ways. You are extroverted, sensing, feeling, and perceiving.\", \n",
    "            \"ESFP_prompt4\",\n",
    "           [\"ESFP\", history[1][\"content\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b1768a-5f7a-4e1e-b2d4-5d6c4ab67926",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8c7ab96-bab0-4b17-a13e-101f1e149944",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Extra: LangChain Setup Attempt (failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfb91c3-3ebc-41cd-bc67-718ac50045c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b125a9656494ffd9975f1e2e2b57f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "hf = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_id,\n",
    "    task=\"text-generation\",\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 100},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12f5ae-4960-4557-9c03-19e7ae1fcde4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
